
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Deteksi Outlier dengan K-Nearest Neighbors (KNN) dalam Data Understanding &#8212; Penambangan Data</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'KNN';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Deteksi Outlier dengan metode Local Outlier Factor (LOF) dalam Data Understanding" href="LOF.html" />
    <link rel="prev" title="OUTLINER DETEKSI" href="Tugas2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/Pas Foto BG Merah.jpeg" class="logo__image only-light" alt="Penambangan Data - Home"/>
    <script>document.write(`<img src="_static/Pas Foto BG Merah.jpeg" class="logo__image only-dark" alt="Penambangan Data - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Penambangan Data
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="tugas1.html">Tugas 1 Pendata</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="Tugas2.html"><strong>OUTLINER DETEKSI</strong></a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#"><strong>Deteksi Outlier dengan K-Nearest Neighbors (KNN) dalam Data Understanding</strong></a></li>





<li class="toctree-l2"><a class="reference internal" href="LOF.html"><strong>Deteksi Outlier dengan metode Local Outlier Factor (LOF) dalam Data Understanding</strong></a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FKNN.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/KNN.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Deteksi Outlier dengan K-Nearest Neighbors (KNN) dalam Data Understanding</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#"><strong>Deteksi Outlier dengan K-Nearest Neighbors (KNN) dalam Data Understanding</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mengapa-knn-bisa-digunakan-untuk-deteksi-outlier">1. Mengapa KNN Bisa Digunakan untuk Deteksi Outlier?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#langkah-langkah-deteksi-outlier-dengan-knn">2. Langkah-Langkah Deteksi Outlier dengan KNN</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#code-untuk-menampilkan-semua-data-dan-grafiknya"><strong>Code Untuk Menampilkan Semua Data dan Grafiknya</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cara-perhitungan"><strong>1. Cara Perhitungan</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cara-kerja-perhitungan"><strong>2. Cara Kerja Perhitungan</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kegunaan-kode"><strong>3. Kegunaan Kode</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#memisahkan-outlier-dari-data"><strong>Memisahkan Outlier dari Data</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><strong>1. Cara Perhitungan</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><strong>2. Cara Kerja Perhitungan</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#menghitung-akurasi-data"><strong>Menghitung Akurasi Data</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#visualisasi-data"><strong>Visualisasi Data</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#persiapan-data">1. <strong>Persiapan Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#membagi-data-training-dan-testing"><strong>2. Membagi Data Training dan Testing</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pembuatan-dan-pelatihan-model-knn"><strong>3. Pembuatan dan Pelatihan Model KNN</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluasi-model"><strong>4. Evaluasi Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualisasi-decision-boundary"><strong>5. Visualisasi Decision Boundary</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#implementasi-naive-bayes-pad-data"><strong>Implementasi Naive Bayes pad Data</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><strong>1. Persiapan Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pelatihan-dan-evaluasi-model"><strong>2. Pelatihan dan Evaluasi Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualisasi-confusion-matrix"><strong>3. Visualisasi Confusion Matrix</strong></a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="deteksi-outlier-dengan-k-nearest-neighbors-knn-dalam-data-understanding">
<h1><strong>Deteksi Outlier dengan K-Nearest Neighbors (KNN) dalam Data Understanding</strong><a class="headerlink" href="#deteksi-outlier-dengan-k-nearest-neighbors-knn-dalam-data-understanding" title="Link to this heading">#</a></h1>
<section id="mengapa-knn-bisa-digunakan-untuk-deteksi-outlier">
<h2>1. Mengapa KNN Bisa Digunakan untuk Deteksi Outlier?<a class="headerlink" href="#mengapa-knn-bisa-digunakan-untuk-deteksi-outlier" title="Link to this heading">#</a></h2>
<p>K-Nearest Neighbors (KNN) bisa digunakan untuk deteksi outlier karena konsep utamanya yang berbasis kedekatan atau jarak antar data. Berikut adalah alasan utama mengapa KNN dapat digunakan dalam deteksi outlier:</p>
<p>a. <strong>Mengukur Jarak Antar Titik Data</strong></p>
<ul class="simple">
<li><p>KNN menggunakan metrik jarak (misalnya, Euclidean, Manhattan, atau lainnya) untuk menentukan seberapa dekat suatu titik dengan titik-titik lain dalam dataset.</p></li>
<li><p>Outlier biasanya memiliki jarak yang lebih jauh dari sebagian besar titik data lainnya.</p></li>
</ul>
<p>b. <strong>Density-Based Outlier Detection</strong></p>
<ul class="simple">
<li><p>Dalam metode seperti k-Nearest Neighbor Distance, sebuah titik dianggap sebagai outlier jika jaraknya ke k-tetangga terdekatnya jauh lebih besar dibandingkan titik lain dalam dataset.</p></li>
<li><p>Titik yang memiliki jumlah tetangga yang sangat sedikit dalam radius tertentu juga bisa dianggap sebagai outlier.</p></li>
</ul>
<p>c. <strong>Local Outlier Factor (LOF)</strong></p>
<ul class="simple">
<li><p>LOF adalah metode berbasis KNN yang membandingkan kerapatan lokal suatu titik dengan kerapatan titik lain di sekitarnya.</p></li>
<li><p>Jika suatu titik memiliki kepadatan yang jauh lebih rendah dibandingkan tetangga-tetangganya, maka titik tersebut bisa dianggap sebagai outlier.</p></li>
</ul>
<p>d. <strong>Non-parametric Approach</strong></p>
<ul class="simple">
<li><p>KNN tidak membuat asumsi tentang distribusi data, sehingga cocok untuk mendeteksi outlier pada dataset yang tidak mengikuti distribusi tertentu.</p></li>
</ul>
<ol class="arabic simple" start="6">
<li><p><strong>Fleksibilitas dalam Multidimensional Space</strong></p></li>
</ol>
<ul class="simple">
<li><p>KNN dapat bekerja dengan baik pada data berdimensi tinggi karena hanya bergantung pada metrik jarak tanpa perlu melakukan pemodelan eksplisit.
Contoh Penggunaan KNN untuk Deteksi Outlier:</p></li>
<li><p>Anomali dalam Data Keuangan: Mendeteksi transaksi penipuan dengan mengidentifikasi transaksi yang jauh berbeda dari pola normal.</p></li>
<li><p>Pendeteksian Kegagalan Mesin: Mengidentifikasi sensor yang menunjukkan perilaku tidak biasa.</p></li>
<li><p>Analisis Data Medis: Mendeteksi pasien dengan kondisi kesehatan ekstrem yang berbeda dari mayoritas.</p></li>
</ul>
</section>
<section id="langkah-langkah-deteksi-outlier-dengan-knn">
<h2>2. Langkah-Langkah Deteksi Outlier dengan KNN<a class="headerlink" href="#langkah-langkah-deteksi-outlier-dengan-knn" title="Link to this heading">#</a></h2>
<p>a. <strong>Persiapan Dataset</strong></p>
<ul class="simple">
<li><p>Pastikan dataset sudah bersih dan siap digunakan.
Jika terdapat data kategori, lakukan encoding agar bisa digunakan dalam perhitungan jarak.</p></li>
<li><p>Normalisasi atau standardisasi data (misalnya menggunakan Min-Max Scaling atau StandardScaler) agar setiap fitur memiliki skala yang seragam.</p></li>
</ul>
<p>b. <strong>Tentukan Metrik Jarak</strong></p>
<ul class="simple">
<li><p>Pilih metrik jarak yang sesuai, misalnya:</p></li>
<li><p>Euclidean Distance (jarak garis lurus antar titik)</p></li>
<li><p>Manhattan Distance (jarak berdasarkan sumbu X dan Y)</p></li>
<li><p>Minkowski Distance (generalized distance metric)</p></li>
</ul>
<p>c. <strong>Tentukan Nilai k (Jumlah Tetangga Terdekat)</strong></p>
<ul class="simple">
<li><p>Nilai k harus cukup besar untuk menangkap pola distribusi data, tetapi tidak terlalu besar agar tetap sensitif terhadap outlier.</p></li>
<li><p>Umumnya, k dipilih antara 5 hingga 10 tergantung pada ukuran dataset.</p></li>
</ul>
<p>d, <strong>Hitung Jarak ke k Tetangga Terdekat</strong></p>
<ul class="simple">
<li><p>Untuk setiap titik data, hitung jarak ke k tetangga terdekatnya.</p></li>
<li><p>Simpan nilai jarak rata-rata atau jarak maksimum dari titik tersebut ke k tetangganya.</p></li>
</ul>
<p>e. <strong>Identifikasi Outlier</strong></p>
<p>f. <strong>Visualisasi Hasil</strong></p>
<ul class="simple">
<li><p>Jika dataset memiliki dua atau tiga dimensi, gunakan scatter plot untuk melihat penyebaran outlier.</p></li>
<li><p>Untuk dataset berdimensi tinggi, gunakan metode seperti Principal Component Analysis (PCA) untuk mereduksi dimensi sebelum visualisasi.</p></li>
</ul>
<p>g. <strong>Evaluasi dan Penyesuaian</strong></p>
<p>Sesuaikan nilai k atau metrik jarak jika hasil deteksi outlier belum optimal.
Coba metode tambahan seperti DBSCAN atau Isolation Forest untuk membandingkan performa deteksi outlier.</p>
<p><strong>Kesimpulan</strong><br />
KNN dapat digunakan dalam tahap Data Understanding untuk mendeteksi outlier dengan menganalisis jarak antara suatu titik dengan tetangga terdekatnya. Dengan metode seperti K-Nearest Neighbor Distance dan Local Outlier Factor (LOF), kita dapat menandai data yang memiliki kepadatan rendah atau jauh dari data lainnya sebagai outlier. Pemilihan nilai K yang tepat dan metrik jarak yang sesuai menjadi faktor penting untuk keberhasilan deteksi outlier dengan KNN.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">pip</span> install pymysql
<span class="o">%</span><span class="k">pip</span> install psycopg2
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: pymysql in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.1.1)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> A new release of pip is available: <span class=" -Color -Color-Red">24.3.1</span> -&gt; <span class=" -Color -Color-Green">25.0.1</span>
<span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> To update, run: <span class=" -Color -Color-Green">python3 -m pip install --upgrade pip</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Note: you may need to restart the kernel to use updated packages.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: psycopg2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.9.10)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> A new release of pip is available: <span class=" -Color -Color-Red">24.3.1</span> -&gt; <span class=" -Color -Color-Green">25.0.1</span>
<span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> To update, run: <span class=" -Color -Color-Green">python3 -m pip install --upgrade pip</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Note: you may need to restart the kernel to use updated packages.
</pre></div>
</div>
</div>
</div>
<p>Kode diatas digunakan untuk menginstal dua pustaka Python, yaitu <code class="docutils literal notranslate"><span class="pre">pymysql</span></code> dan <code class="docutils literal notranslate"><span class="pre">psycopg2</span></code>, yang berfungsi sebagai konektor untuk basis data (database). Pustaka <code class="docutils literal notranslate"><span class="pre">pymysql</span></code> digunakan untuk menghubungkan aplikasi Python dengan database MySQL atau MariaDB menggunakan protokol MySQL asli. Sementara itu, <code class="docutils literal notranslate"><span class="pre">psycopg2</span></code> merupakan pustaka yang digunakan untuk menghubungkan Python dengan database PostgreSQL, menyediakan antarmuka yang efisien untuk menjalankan perintah SQL dan berinteraksi dengan database tersebut. Kedua perintah ini menggunakan <strong>magic command</strong> <code class="docutils literal notranslate"><span class="pre">%pip</span></code>, yang umumnya digunakan dalam lingkungan Jupyter Notebook untuk memastikan bahwa paket yang diperlukan diinstal dalam kernel yang sedang berjalan.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="code-untuk-menampilkan-semua-data-dan-grafiknya">
<h1><strong>Code Untuk Menampilkan Semua Data dan Grafiknya</strong><a class="headerlink" href="#code-untuk-menampilkan-semua-data-dan-grafiknya" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">psycopg2</span>
<span class="kn">import</span> <span class="nn">pymysql</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>

<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">float_format</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span>  <span class="c1"># Format angka desimal agar lebih rapi</span>

<span class="c1"># Fungsi untuk mengambil data dari PostgreSQL</span>
<span class="k">def</span> <span class="nf">get_pg_data</span><span class="p">():</span>
    <span class="n">conn</span> <span class="o">=</span> <span class="n">psycopg2</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span>
        <span class="n">host</span><span class="o">=</span><span class="s2">&quot;pg-292ef6f0-postgesql.d.aivencloud.com&quot;</span><span class="p">,</span>
        <span class="n">user</span><span class="o">=</span><span class="s2">&quot;avnadmin&quot;</span><span class="p">,</span>
        <span class="n">password</span><span class="o">=</span><span class="s2">&quot;AVNS_iFCt7xp02vTczdlhooY&quot;</span><span class="p">,</span>
        <span class="n">database</span><span class="o">=</span><span class="s2">&quot;defaultdb&quot;</span><span class="p">,</span>
        <span class="n">port</span><span class="o">=</span><span class="mi">16905</span>
    <span class="p">)</span>
    <span class="n">cursor</span> <span class="o">=</span> <span class="n">conn</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span>
    <span class="n">cursor</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM firmandani.postgree&quot;</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">cursor</span><span class="o">.</span><span class="n">fetchall</span><span class="p">()</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">desc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">desc</span> <span class="ow">in</span> <span class="n">cursor</span><span class="o">.</span><span class="n">description</span><span class="p">]</span>
    <span class="n">cursor</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">conn</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># Fungsi untuk mengambil data dari MySQL</span>
<span class="k">def</span> <span class="nf">get_mysql_data</span><span class="p">():</span>
    <span class="n">conn</span> <span class="o">=</span> <span class="n">pymysql</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span>
        <span class="n">host</span><span class="o">=</span><span class="s2">&quot;mysql-ef66224-mmsql.d.aivencloud.com&quot;</span><span class="p">,</span>
        <span class="n">user</span><span class="o">=</span><span class="s2">&quot;avnadmin&quot;</span><span class="p">,</span>
        <span class="n">password</span><span class="o">=</span><span class="s2">&quot;AVNS_NrOpqQJhytxwdcC6shv&quot;</span><span class="p">,</span>
        <span class="n">database</span><span class="o">=</span><span class="s2">&quot;defaultdb&quot;</span><span class="p">,</span>
        <span class="n">port</span><span class="o">=</span><span class="mi">21910</span>
    <span class="p">)</span>
    <span class="n">cursor</span> <span class="o">=</span> <span class="n">conn</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span>
    <span class="n">cursor</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM flowers&quot;</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">cursor</span><span class="o">.</span><span class="n">fetchall</span><span class="p">()</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">desc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">desc</span> <span class="ow">in</span> <span class="n">cursor</span><span class="o">.</span><span class="n">description</span><span class="p">]</span>
    <span class="n">cursor</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">conn</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># Ambil data dari kedua database</span>
<span class="n">df_postgresql</span> <span class="o">=</span> <span class="n">get_pg_data</span><span class="p">()</span>
<span class="n">df_mysql</span> <span class="o">=</span> <span class="n">get_mysql_data</span><span class="p">()</span>
<span class="n">df_postgresql</span> <span class="o">=</span> <span class="n">df_postgresql</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;Class&#39;</span><span class="p">:</span> <span class="s1">&#39;class&#39;</span><span class="p">})</span>

<span class="c1"># Gabungkan berdasarkan kolom &#39;id&#39;</span>
<span class="n">df_merged</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df_mysql</span><span class="p">,</span> <span class="n">df_postgresql</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">],</span> <span class="n">how</span><span class="o">=</span><span class="s2">&quot;inner&quot;</span><span class="p">)</span>

<span class="c1"># Ambil data fitur numerik</span>
<span class="n">feature_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;petal length&quot;</span><span class="p">,</span> <span class="s2">&quot;petal width&quot;</span><span class="p">,</span> <span class="s2">&quot;sepal length&quot;</span><span class="p">,</span> <span class="s2">&quot;sepal width&quot;</span><span class="p">]</span>
<span class="n">data_values</span> <span class="o">=</span> <span class="n">df_merged</span><span class="p">[</span><span class="n">feature_columns</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># KNN Outlier Detection</span>
<span class="k">def</span> <span class="nf">knn_outlier_detection</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">90</span><span class="p">):</span>
    <span class="n">neigh</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">distances</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">neigh</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">avg_distances</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Ambil jarak k-terjauh sebagai skor</span>
    <span class="k">return</span> <span class="n">avg_distances</span>

<span class="c1"># Hitung K-NN distance</span>
<span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;knn_distance&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">knn_outlier_detection</span><span class="p">(</span><span class="n">data_values</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>

<span class="c1"># Tentukan threshold sebagai nilai rata-rata + 2 standar deviasi</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;knn_distance&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;knn_distance&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;outlier_knn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;knn_distance&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threshold</span>

<span class="c1"># Hitung jumlah data sebelum dan sesudah penghapusan outlier</span>
<span class="n">total_data_awal</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_merged</span><span class="p">)</span>
<span class="n">df_filtered</span> <span class="o">=</span> <span class="n">df_merged</span><span class="p">[</span><span class="o">~</span><span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;outlier_knn&quot;</span><span class="p">]]</span>
<span class="n">total_data_akhir</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_filtered</span><span class="p">)</span>

<span class="c1"># Cetak hasil setelah outlier dihapus</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;DATASET SETELAH OUTLIER DIHAPUS&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_filtered</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Jumlah data sebelum outlier dihapus: </span><span class="si">{</span><span class="n">total_data_awal</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Jumlah data setelah outlier dihapus : </span><span class="si">{</span><span class="n">total_data_akhir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

<span class="c1"># Visualisasi setelah outlier dihapus</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">df_filtered</span><span class="p">[</span><span class="s2">&quot;sepal length&quot;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">df_filtered</span><span class="p">[</span><span class="s2">&quot;sepal width&quot;</span><span class="p">],</span>
    <span class="n">hue</span><span class="o">=</span><span class="n">df_filtered</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">],</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;deep&quot;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Data Sepal setelah Outlier Dihapus&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">df_filtered</span><span class="p">[</span><span class="s2">&quot;petal length&quot;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">df_filtered</span><span class="p">[</span><span class="s2">&quot;petal width&quot;</span><span class="p">],</span>
    <span class="n">hue</span><span class="o">=</span><span class="n">df_filtered</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">],</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;deep&quot;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Data Petal setelah Outlier Dihapus&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--------------------------------------------------
DATASET SETELAH OUTLIER DIHAPUS
--------------------------------------------------
 id           class  petal length  petal width  sepal length  sepal width  knn_distance  outlier_knn
  1     Iris-setosa          1.30         0.20          5.10         3.50          4.12        False
  2     Iris-setosa        122.00       123.00        223.00       121.00        300.02        False
  3     Iris-setosa        234.00       241.00        121.00       142.00        378.32        False
  7     Iris-setosa          1.40         0.30          4.60         3.40          4.11        False
  8     Iris-setosa          1.00         0.20          5.00         3.40          4.38        False
  9     Iris-setosa          4.00         0.20          5.00         2.90          2.66        False
 10     Iris-setosa          1.50         0.30          4.90         3.10          3.91        False
 11     Iris-setosa          1.50         0.20          5.40         3.70          3.89        False
 12     Iris-setosa          1.60         0.30          4.80         3.40          3.87        False
 13     Iris-setosa          4.30         0.10          4.80         3.00          2.92        False
 14     Iris-setosa          7.50         0.10          4.30         3.00          4.35        False
 15     Iris-setosa          1.20         0.20          5.80         4.00          4.16        False
 16     Iris-setosa          1.50         0.40          5.70         4.40          3.96        False
 17     Iris-setosa          1.30         0.40          5.40         3.90          4.05        False
 18     Iris-setosa          1.40         0.30          5.10         3.50          3.98        False
 19     Iris-setosa          1.70         0.30          5.70         3.80          3.65        False
 20     Iris-setosa          1.50         0.30          5.10         3.80          3.96        False
 21     Iris-setosa          1.70         0.20          5.40         3.40          3.67        False
 22     Iris-setosa          1.50         0.40          5.10         3.70          3.90        False
 23     Iris-setosa          1.00         0.20          4.60         3.60          4.51        False
 24     Iris-setosa          1.70         0.50          5.10         3.30          3.61        False
 25     Iris-setosa          1.90         0.20          4.80         3.40          3.66        False
 26     Iris-setosa          1.60         0.20          5.00         3.00          3.81        False
 27     Iris-setosa          1.60         0.40          5.00         3.40          3.77        False
 28     Iris-setosa          1.50         0.20          5.20         3.50          3.92        False
 29     Iris-setosa          1.40         0.20          5.20         3.40          3.99        False
 30     Iris-setosa          1.60         0.20          4.70         3.20          3.93        False
 31     Iris-setosa          1.60         0.20          4.80         3.10          3.89        False
 32     Iris-setosa          1.50         0.40          5.40         3.40          3.77        False
 33     Iris-setosa          1.50         0.10          5.20         4.10          4.10        False
 34     Iris-setosa          1.40         0.20          5.50         4.20          4.08        False
 35     Iris-setosa          1.50         0.10          4.90         3.10          3.98        False
 36     Iris-setosa          1.20         0.20          5.00         3.20          4.18        False
 37     Iris-setosa          1.30         0.20          5.50         3.50          4.02        False
 38     Iris-setosa          1.50         0.10          4.90         3.10          3.98        False
 39     Iris-setosa          1.30         0.20          4.40         3.00          4.30        False
 40     Iris-setosa          1.50         0.20          5.10         3.40          3.93        False
 41     Iris-setosa          1.30         0.30          5.00         3.50          4.09        False
 42     Iris-setosa          1.30         0.30          4.50         2.30          4.20        False
 43     Iris-setosa          1.30         0.20          4.40         3.20          4.31        False
 44     Iris-setosa          1.60         0.60          5.00         3.50          3.71        False
 45     Iris-setosa          1.90         0.40          5.10         3.80          3.58        False
 46     Iris-setosa          1.40         0.30          4.80         3.00          4.02        False
 47     Iris-setosa          1.60         0.20          5.10         3.80          3.92        False
 48     Iris-setosa          1.40         0.20          4.60         3.20          4.14        False
 49     Iris-setosa          1.50         0.20          5.30         3.70          3.92        False
 50     Iris-setosa          1.40         0.20          5.00         3.30          4.01        False
 51 Iris-versicolor        124.00       231.00        210.00       201.00        385.09        False
 54 Iris-versicolor        322.00       140.00        122.00       110.00        381.35        False
 56 Iris-versicolor          4.50         1.30          5.70         2.80          3.01        False
 57 Iris-versicolor          4.70         1.60          6.30         3.30          2.52        False
 58 Iris-versicolor          3.30         1.00          4.90         2.40          2.43        False
 59 Iris-versicolor          4.60         1.30          6.60         2.90          2.50        False
 60 Iris-versicolor          3.90         1.40          5.20         2.70          2.69        False
 61 Iris-versicolor          3.50         1.00          5.00         2.00          2.70        False
 62 Iris-versicolor          4.20         1.50          5.90         3.00          2.89        False
 63 Iris-versicolor          4.00         1.00          6.00         2.20          2.85        False
 64 Iris-versicolor          4.70         1.40          6.10         2.90          2.70        False
 65 Iris-versicolor          3.60         1.30          5.60         2.90          2.47        False
 66 Iris-versicolor          4.40         1.40          6.70         3.10          2.60        False
 67 Iris-versicolor          4.50         1.50          5.60         3.00          3.03        False
 68 Iris-versicolor          4.10         1.00          5.80         2.70          2.76        False
 69 Iris-versicolor          4.50         1.50          6.20         2.20          2.97        False
 70 Iris-versicolor          3.90         1.10          5.60         2.50          2.66        False
 71 Iris-versicolor          4.80         1.80          5.90         3.20          2.66        False
 72 Iris-versicolor          4.00         1.30          6.10         2.80          2.77        False
 73 Iris-versicolor          4.90         1.50          6.30         2.50          2.57        False
 74 Iris-versicolor          4.70         1.20          6.10         2.80          2.79        False
 75 Iris-versicolor          4.30         1.30          6.40         2.90          2.82        False
 76 Iris-versicolor          4.40         1.40          6.60         3.00          2.63        False
 77 Iris-versicolor          4.80         1.40          6.80         2.80          2.46        False
 78 Iris-versicolor          5.00         1.70          6.70         3.00          2.60        False
 79 Iris-versicolor          4.50         1.50          6.00         2.90          2.88        False
 80 Iris-versicolor          3.50         1.00          5.70         2.60          2.44        False
 81 Iris-versicolor          3.80         1.10          5.50         2.40          2.64        False
 82 Iris-versicolor          3.70         1.00          5.50         2.40          2.56        False
 83 Iris-versicolor          3.90         1.20          5.80         2.70          2.65        False
 84 Iris-versicolor          5.10         1.60          6.00         2.70          2.57        False
 85 Iris-versicolor          4.50         1.50          5.40         3.00          3.00        False
 86 Iris-versicolor          4.50         1.60          6.00         3.40          2.87        False
 87 Iris-versicolor          4.70         1.50          6.70         3.10          2.44        False
 88 Iris-versicolor          4.40         1.30          6.30         2.30          3.05        False
 89 Iris-versicolor          4.10         1.30          5.60         3.00          2.75        False
 90 Iris-versicolor          4.00         1.30          5.50         2.50          2.76        False
 91 Iris-versicolor          4.40         1.20          5.50         2.60          2.99        False
 92 Iris-versicolor          4.60         1.40          6.10         3.00          2.73        False
 93 Iris-versicolor          4.00         1.20          5.80         2.60          2.75        False
 94 Iris-versicolor          3.30         1.00          5.00         2.30          2.45        False
 95 Iris-versicolor          4.20         1.30          5.60         2.70          2.87        False
 96 Iris-versicolor          4.20         1.20          5.70         3.00          2.77        False
 97 Iris-versicolor          4.20         1.30          5.70         2.90          2.79        False
 98 Iris-versicolor          4.30         1.30          6.20         2.90          2.93        False
 99 Iris-versicolor          3.00         1.10          5.10         2.50          2.37        False
100 Iris-versicolor          4.10         1.30          5.00         2.60          2.81        False
101  Iris-virginica        121.00       134.00        122.00       112.00        238.42        False
102  Iris-virginica        134.00       231.00        221.00       110.00        357.21        False
103  Iris-virginica        123.00       234.00        113.00       112.00        303.02        False
106  Iris-virginica          6.60         2.10          7.60         3.00          4.31        False
107  Iris-virginica          4.50         1.70          4.90         2.50          3.26        False
108  Iris-virginica          6.30         1.80          7.30         2.90          3.82        False
109  Iris-virginica          5.80         1.80          6.70         2.50          3.13        False
110  Iris-virginica          6.10         2.50          7.20         3.60          4.06        False
111  Iris-virginica          5.10         2.00          6.50         3.20          2.70        False
112  Iris-virginica          5.30         1.90          6.40         2.70          2.63        False
113  Iris-virginica          5.50         2.10          6.80         3.00          3.13        False
114  Iris-virginica          5.00         2.00          5.70         2.50          2.78        False
115  Iris-virginica          5.10         2.40          5.80         2.80          2.64        False
116  Iris-virginica          5.30         2.30          6.40         3.20          2.91        False
117  Iris-virginica          5.50         1.80          6.50         3.00          2.87        False
118  Iris-virginica          6.70         2.20          7.70         3.80          4.71        False
119  Iris-virginica          6.90         2.30          7.70         2.60          4.57        False
120  Iris-virginica          5.00         1.50          6.00         2.20          2.70        False
121  Iris-virginica          5.70         2.30          6.90         3.20          3.45        False
122  Iris-virginica          4.90         2.00          5.60         2.80          2.92        False
123  Iris-virginica          6.70         2.00          7.70         2.80          4.38        False
124  Iris-virginica          4.90         1.80          6.30         2.70          2.46        False
125  Iris-virginica          5.70         2.10          6.70         3.30          3.30        False
126  Iris-virginica          6.00         1.80          7.20         3.20          3.69        False
127  Iris-virginica          4.80         1.80          6.20         2.80          2.55        False
128  Iris-virginica          4.90         1.80          6.10         3.00          2.48        False
129  Iris-virginica          5.60         2.10          6.40         2.80          2.95        False
130  Iris-virginica          5.80         1.60          7.20         3.00          3.46        False
131  Iris-virginica          6.10         1.90          7.40         2.80          3.83        False
132  Iris-virginica          6.40         2.00          7.90         3.80          4.59        False
133  Iris-virginica          5.60         2.20          6.40         2.80          2.99        False
134  Iris-virginica          5.10         1.50          6.30         2.80          2.42        False
135  Iris-virginica          5.60         1.40          6.10         2.60          2.60        False
136  Iris-virginica          6.10         2.30          7.70         3.00          4.16        False
137  Iris-virginica          5.60         2.40          6.30         3.40          3.19        False
138  Iris-virginica          5.50         1.80          6.40         3.10          2.84        False
139  Iris-virginica          4.80         1.80          6.00         3.00          2.62        False
140  Iris-virginica          5.40         2.10          6.90         3.10          3.14        False
141  Iris-virginica          5.60         2.40          6.70         3.10          3.28        False
142  Iris-virginica          5.10         2.30          6.90         3.10          3.07        False
143  Iris-virginica          5.10         1.90          5.80         2.70          2.65        False
144  Iris-virginica          5.90         2.30          6.80         3.20          3.54        False
145  Iris-virginica          5.70         2.50          6.70         3.30          3.45        False
146  Iris-virginica          5.20         2.30          6.70         3.00          2.98        False
147  Iris-virginica          5.00         1.90          6.30         2.50          2.49        False
148  Iris-virginica          5.20         2.00          6.50         3.00          2.71        False
149  Iris-virginica          5.40         2.30          6.20         3.40          2.96        False
150  Iris-virginica          5.10         1.80          5.90         3.00          2.52        False
--------------------------------------------------
Jumlah data sebelum outlier dihapus: 150
Jumlah data setelah outlier dihapus : 142
--------------------------------------------------
</pre></div>
</div>
<img alt="_images/4458130253c61bcc1f58dc74f85458e2ae3ea7384d0693e47598120c3c62d32e.png" src="_images/4458130253c61bcc1f58dc74f85458e2ae3ea7384d0693e47598120c3c62d32e.png" />
<img alt="_images/7adca4faa976f70bfeb49218c0d951d85f20407d3b06d3e8ea61cef5809161ad.png" src="_images/7adca4faa976f70bfeb49218c0d951d85f20407d3b06d3e8ea61cef5809161ad.png" />
</div>
</div>
<section id="cara-perhitungan">
<h2><strong>1. Cara Perhitungan</strong><a class="headerlink" href="#cara-perhitungan" title="Link to this heading">#</a></h2>
<p>a Mengambil Data dari Database PostgreSQL &amp; MySQL</p>
<ol class="arabic simple">
<li><p>Kode mengambil data dari PostgreSQL (get_pg_data()) dan MySQL (get_mysql_data()).</p></li>
<li><p>Data dari kedua database digabungkan berdasarkan kolom id dan class.</p></li>
</ol>
<p>b Menghitung Jarak KNN</p>
<ol class="arabic simple">
<li><p>Menentukan fitur numerik yang akan digunakan dalam analisis</p></li>
</ol>
<ul class="simple">
<li><p>Hanya kolom numerik yang dipakai dalam deteksi outlier.</p></li>
</ul>
<p>c. Melatih model KNN dengan k=90:</p>
<ol class="arabic simple">
<li><p>NearestNeighbors(n_neighbors=90) berarti setiap titik data akan dihitung jarak ke 90 tetangga terdekatnya.</p></li>
<li><p>kneighbors() menghitung jarak setiap titik ke 90 tetangganya.</p></li>
<li><p>Jarak ke-90 tetangga dipilih sebagai ukuran outlier. Semakin besar nilai ini, semakin mungkin data itu adalah outlier.</p></li>
</ol>
<p>d  Menentukan Outlier</p>
<ol class="arabic simple">
<li><p>Menentukan threshold outlier:</p></li>
</ol>
<ul class="simple">
<li><p>Threshold ditentukan sebagai rata-rata jarak + 2 standar deviasi.</p></li>
<li><p>Mengapa +2 standar deviasi?</p></li>
<li><p>Dalam distribusi normal, 95% data berada dalam 2 standar deviasi dari rata-rata.</p></li>
<li><p>Data yang memiliki jarak lebih dari ini dianggap outlier.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>Menandai data sebagai outlier atau bukan:</p></li>
</ol>
<ul class="simple">
<li><p>Jika jarak ke-90 lebih besar dari threshold, maka data tersebut outlier.</p></li>
</ul>
<p>e Menghapus Outlier</p>
<ol class="arabic simple">
<li><p>Hanya menyimpan data yang bukan outlier:
python</p></li>
</ol>
<ul class="simple">
<li><p>Data yang bukan outlier (False) dipertahankan.</p></li>
</ul>
</section>
<section id="cara-kerja-perhitungan">
<h2><strong>2. Cara Kerja Perhitungan</strong><a class="headerlink" href="#cara-kerja-perhitungan" title="Link to this heading">#</a></h2>
<p>Kode ini menggunakan metode K-Nearest Neighbors (KNN) Outlier Detection yang bekerja dengan mengukur jarak setiap titik ke tetangga terdekatnya. Berikut adalah langkah-langkahnya:</p>
<ol class="arabic simple">
<li><p>Ambil data dari dua database (PostgreSQL &amp; MySQL).</p></li>
<li><p>Gabungkan data berdasarkan id dan class.</p></li>
<li><p>Pilih fitur numerik yang digunakan untuk analisis outlier.</p></li>
<li><p>Gunakan KNN untuk menghitung jarak ke 90 tetangga terdekat untuk setiap titik data.</p></li>
<li><p>Gunakan threshold rata-rata + 2 standar deviasi untuk menentukan outlier.</p></li>
<li><p>Hapus outlier dan simpan data bersih.</p></li>
<li><p>Visualisasikan data sebelum dan sesudah outlier dihapus.</p></li>
</ol>
</section>
<section id="kegunaan-kode">
<h2><strong>3. Kegunaan Kode</strong><a class="headerlink" href="#kegunaan-kode" title="Link to this heading">#</a></h2>
<p>Kode ini berguna untuk:</p>
<ol class="arabic simple">
<li><p>Mengidentifikasi outlier dalam dataset berdasarkan jarak ke tetangga terdekat.</p></li>
<li><p>Membersihkan dataset dari data yang tidak sesuai atau ekstrem.</p></li>
<li><p>Meningkatkan akurasi model machine learning dengan menghapus data anomali.</p></li>
<li><p>Memudahkan analisis data dengan visualisasi yang lebih bersih.</p></li>
</ol>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="memisahkan-outlier-dari-data">
<h1><strong>Memisahkan Outlier dari Data</strong><a class="headerlink" href="#memisahkan-outlier-dari-data" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">psycopg2</span>
<span class="kn">import</span> <span class="nn">pymysql</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span> <span class="p">,</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">LocalOutlierFactor</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">LabelEncoder</span><span class="p">,</span> <span class="n">Normalizer</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>

<span class="k">def</span> <span class="nf">get_pg_data</span><span class="p">():</span>
    <span class="n">conn</span> <span class="o">=</span> <span class="n">psycopg2</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span>
        <span class="n">host</span><span class="o">=</span><span class="s2">&quot;pg-292ef6f0-postgesql.d.aivencloud.com&quot;</span><span class="p">,</span>
        <span class="n">user</span><span class="o">=</span><span class="s2">&quot;avnadmin&quot;</span><span class="p">,</span>
        <span class="n">password</span><span class="o">=</span><span class="s2">&quot;AVNS_iFCt7xp02vTczdlhooY&quot;</span><span class="p">,</span>
        <span class="n">database</span><span class="o">=</span><span class="s2">&quot;defaultdb&quot;</span><span class="p">,</span>
        <span class="n">port</span><span class="o">=</span><span class="mi">16905</span>
    <span class="p">)</span>
    <span class="n">cursor</span> <span class="o">=</span> <span class="n">conn</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span>
    <span class="n">cursor</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM firmandani.postgree&quot;</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">cursor</span><span class="o">.</span><span class="n">fetchall</span><span class="p">()</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">desc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">desc</span> <span class="ow">in</span> <span class="n">cursor</span><span class="o">.</span><span class="n">description</span><span class="p">]</span>
    <span class="n">cursor</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">conn</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_mysql_data</span><span class="p">():</span>
    <span class="n">conn</span> <span class="o">=</span> <span class="n">pymysql</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span>
        <span class="n">host</span><span class="o">=</span><span class="s2">&quot;mysql-ef66224-mmsql.d.aivencloud.com&quot;</span><span class="p">,</span>
        <span class="n">user</span><span class="o">=</span><span class="s2">&quot;avnadmin&quot;</span><span class="p">,</span>
        <span class="n">password</span><span class="o">=</span><span class="s2">&quot;AVNS_NrOpqQJhytxwdcC6shv&quot;</span><span class="p">,</span>
        <span class="n">database</span><span class="o">=</span><span class="s2">&quot;defaultdb&quot;</span><span class="p">,</span>
        <span class="n">port</span><span class="o">=</span><span class="mi">21910</span>
    <span class="p">)</span>
    <span class="n">cursor</span> <span class="o">=</span> <span class="n">conn</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span>
    <span class="n">cursor</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM flowers&quot;</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">cursor</span><span class="o">.</span><span class="n">fetchall</span><span class="p">()</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">desc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">desc</span> <span class="ow">in</span> <span class="n">cursor</span><span class="o">.</span><span class="n">description</span><span class="p">]</span>
    <span class="n">cursor</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">conn</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># Ambil data dari kedua database</span>
<span class="n">df_postgresql</span> <span class="o">=</span> <span class="n">get_pg_data</span><span class="p">()</span>
<span class="n">df_mysql</span> <span class="o">=</span> <span class="n">get_mysql_data</span><span class="p">()</span>
<span class="n">df_postgresql</span> <span class="o">=</span> <span class="n">df_postgresql</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;Class&#39;</span><span class="p">:</span> <span class="s1">&#39;class&#39;</span><span class="p">})</span>

<span class="c1"># Gabungkan berdasarkan kolom &#39;id&#39; dan &#39;class&#39;</span>
<span class="n">df_merge</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df_mysql</span><span class="p">,</span> <span class="n">df_postgresql</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">],</span> <span class="n">how</span><span class="o">=</span><span class="s2">&quot;inner&quot;</span><span class="p">)</span>

<span class="c1"># Ambil data fitur numerik tanpa kolom &#39;class&#39;</span>
<span class="n">feature_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;petal length&quot;</span><span class="p">,</span> <span class="s2">&quot;petal width&quot;</span><span class="p">,</span> <span class="s2">&quot;sepal length&quot;</span><span class="p">,</span> <span class="s2">&quot;sepal width&quot;</span><span class="p">]</span>
<span class="n">data_values</span> <span class="o">=</span> <span class="n">df_merge</span><span class="p">[</span><span class="n">feature_columns</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># Inisialisasi model LOF</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">LocalOutlierFactor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">data_values</span><span class="p">)</span>

<span class="c1"># Tambahkan hasil label ke dataframe</span>
<span class="n">df_merge</span><span class="p">[</span><span class="s2">&quot;outlier_label&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">label</span>

<span class="c1"># Cetak hasil dengan ID dan class</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_merge</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="n">num_outliers</span> <span class="o">=</span> <span class="p">(</span><span class="n">label</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Jumlah outlier: </span><span class="si">{</span><span class="n">num_outliers</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">df_filtered</span> <span class="o">=</span> <span class="n">df_merge</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;outlier_label&quot;</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="n">outliers</span> <span class="o">=</span> <span class="n">df_merge</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;outlier_label&quot;</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Data Outlier:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">outliers</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Jumlah data setelah dihapus : &quot;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">df_filtered</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Data tidak outlier :&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_filtered</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> id           class  petal length  petal width  sepal length  sepal width  outlier_label
  1     Iris-setosa          1.30         0.20          5.10         3.50              1
  2     Iris-setosa        122.00       123.00        223.00       121.00             -1
  3     Iris-setosa        234.00       241.00        121.00       142.00             -1
  4     Iris-setosa        210.00       423.00        135.00      1423.00             -1
  5     Iris-setosa        233.00       230.00        123.00       241.00             -1
  6     Iris-setosa        201.00       221.00        521.00       123.00             -1
  7     Iris-setosa          1.40         0.30          4.60         3.40              1
  8     Iris-setosa          1.00         0.20          5.00         3.40              1
  9     Iris-setosa          4.00         0.20          5.00         2.90              1
 10     Iris-setosa          1.50         0.30          4.90         3.10              1
 11     Iris-setosa          1.50         0.20          5.40         3.70              1
 12     Iris-setosa          1.60         0.30          4.80         3.40              1
 13     Iris-setosa          4.30         0.10          4.80         3.00              1
 14     Iris-setosa          7.50         0.10          4.30         3.00              1
 15     Iris-setosa          1.20         0.20          5.80         4.00              1
 16     Iris-setosa          1.50         0.40          5.70         4.40              1
 17     Iris-setosa          1.30         0.40          5.40         3.90              1
 18     Iris-setosa          1.40         0.30          5.10         3.50              1
 19     Iris-setosa          1.70         0.30          5.70         3.80              1
 20     Iris-setosa          1.50         0.30          5.10         3.80              1
 21     Iris-setosa          1.70         0.20          5.40         3.40              1
 22     Iris-setosa          1.50         0.40          5.10         3.70              1
 23     Iris-setosa          1.00         0.20          4.60         3.60              1
 24     Iris-setosa          1.70         0.50          5.10         3.30              1
 25     Iris-setosa          1.90         0.20          4.80         3.40              1
 26     Iris-setosa          1.60         0.20          5.00         3.00              1
 27     Iris-setosa          1.60         0.40          5.00         3.40              1
 28     Iris-setosa          1.50         0.20          5.20         3.50              1
 29     Iris-setosa          1.40         0.20          5.20         3.40              1
 30     Iris-setosa          1.60         0.20          4.70         3.20              1
 31     Iris-setosa          1.60         0.20          4.80         3.10              1
 32     Iris-setosa          1.50         0.40          5.40         3.40              1
 33     Iris-setosa          1.50         0.10          5.20         4.10              1
 34     Iris-setosa          1.40         0.20          5.50         4.20              1
 35     Iris-setosa          1.50         0.10          4.90         3.10              1
 36     Iris-setosa          1.20         0.20          5.00         3.20              1
 37     Iris-setosa          1.30         0.20          5.50         3.50              1
 38     Iris-setosa          1.50         0.10          4.90         3.10              1
 39     Iris-setosa          1.30         0.20          4.40         3.00              1
 40     Iris-setosa          1.50         0.20          5.10         3.40              1
 41     Iris-setosa          1.30         0.30          5.00         3.50              1
 42     Iris-setosa          1.30         0.30          4.50         2.30              1
 43     Iris-setosa          1.30         0.20          4.40         3.20              1
 44     Iris-setosa          1.60         0.60          5.00         3.50              1
 45     Iris-setosa          1.90         0.40          5.10         3.80              1
 46     Iris-setosa          1.40         0.30          4.80         3.00              1
 47     Iris-setosa          1.60         0.20          5.10         3.80              1
 48     Iris-setosa          1.40         0.20          4.60         3.20              1
 49     Iris-setosa          1.50         0.20          5.30         3.70              1
 50     Iris-setosa          1.40         0.20          5.00         3.30              1
 51 Iris-versicolor        124.00       231.00        210.00       201.00             -1
 52 Iris-versicolor        212.00       201.00        120.00       311.00             -1
 53 Iris-versicolor        310.00       414.00        125.00       144.00             -1
 54 Iris-versicolor        322.00       140.00        122.00       110.00             -1
 55 Iris-versicolor        112.00       312.00        211.00       112.00             -1
 56 Iris-versicolor          4.50         1.30          5.70         2.80              1
 57 Iris-versicolor          4.70         1.60          6.30         3.30              1
 58 Iris-versicolor          3.30         1.00          4.90         2.40              1
 59 Iris-versicolor          4.60         1.30          6.60         2.90              1
 60 Iris-versicolor          3.90         1.40          5.20         2.70              1
 61 Iris-versicolor          3.50         1.00          5.00         2.00              1
 62 Iris-versicolor          4.20         1.50          5.90         3.00              1
 63 Iris-versicolor          4.00         1.00          6.00         2.20              1
 64 Iris-versicolor          4.70         1.40          6.10         2.90              1
 65 Iris-versicolor          3.60         1.30          5.60         2.90              1
 66 Iris-versicolor          4.40         1.40          6.70         3.10              1
 67 Iris-versicolor          4.50         1.50          5.60         3.00              1
 68 Iris-versicolor          4.10         1.00          5.80         2.70              1
 69 Iris-versicolor          4.50         1.50          6.20         2.20              1
 70 Iris-versicolor          3.90         1.10          5.60         2.50              1
 71 Iris-versicolor          4.80         1.80          5.90         3.20              1
 72 Iris-versicolor          4.00         1.30          6.10         2.80              1
 73 Iris-versicolor          4.90         1.50          6.30         2.50              1
 74 Iris-versicolor          4.70         1.20          6.10         2.80              1
 75 Iris-versicolor          4.30         1.30          6.40         2.90              1
 76 Iris-versicolor          4.40         1.40          6.60         3.00              1
 77 Iris-versicolor          4.80         1.40          6.80         2.80              1
 78 Iris-versicolor          5.00         1.70          6.70         3.00              1
 79 Iris-versicolor          4.50         1.50          6.00         2.90              1
 80 Iris-versicolor          3.50         1.00          5.70         2.60              1
 81 Iris-versicolor          3.80         1.10          5.50         2.40              1
 82 Iris-versicolor          3.70         1.00          5.50         2.40              1
 83 Iris-versicolor          3.90         1.20          5.80         2.70              1
 84 Iris-versicolor          5.10         1.60          6.00         2.70              1
 85 Iris-versicolor          4.50         1.50          5.40         3.00              1
 86 Iris-versicolor          4.50         1.60          6.00         3.40              1
 87 Iris-versicolor          4.70         1.50          6.70         3.10              1
 88 Iris-versicolor          4.40         1.30          6.30         2.30              1
 89 Iris-versicolor          4.10         1.30          5.60         3.00              1
 90 Iris-versicolor          4.00         1.30          5.50         2.50              1
 91 Iris-versicolor          4.40         1.20          5.50         2.60              1
 92 Iris-versicolor          4.60         1.40          6.10         3.00              1
 93 Iris-versicolor          4.00         1.20          5.80         2.60              1
 94 Iris-versicolor          3.30         1.00          5.00         2.30              1
 95 Iris-versicolor          4.20         1.30          5.60         2.70              1
 96 Iris-versicolor          4.20         1.20          5.70         3.00              1
 97 Iris-versicolor          4.20         1.30          5.70         2.90              1
 98 Iris-versicolor          4.30         1.30          6.20         2.90              1
 99 Iris-versicolor          3.00         1.10          5.10         2.50              1
100 Iris-versicolor          4.10         1.30          5.00         2.60              1
101  Iris-virginica        121.00       134.00        122.00       112.00             -1
102  Iris-virginica        134.00       231.00        221.00       110.00             -1
103  Iris-virginica        123.00       234.00        113.00       112.00             -1
104  Iris-virginica        210.00       253.00        123.00       221.00             -1
105  Iris-virginica        240.00       124.00        333.00       211.00             -1
106  Iris-virginica          6.60         2.10          7.60         3.00              1
107  Iris-virginica          4.50         1.70          4.90         2.50              1
108  Iris-virginica          6.30         1.80          7.30         2.90              1
109  Iris-virginica          5.80         1.80          6.70         2.50              1
110  Iris-virginica          6.10         2.50          7.20         3.60              1
111  Iris-virginica          5.10         2.00          6.50         3.20              1
112  Iris-virginica          5.30         1.90          6.40         2.70              1
113  Iris-virginica          5.50         2.10          6.80         3.00              1
114  Iris-virginica          5.00         2.00          5.70         2.50              1
115  Iris-virginica          5.10         2.40          5.80         2.80              1
116  Iris-virginica          5.30         2.30          6.40         3.20              1
117  Iris-virginica          5.50         1.80          6.50         3.00              1
118  Iris-virginica          6.70         2.20          7.70         3.80              1
119  Iris-virginica          6.90         2.30          7.70         2.60              1
120  Iris-virginica          5.00         1.50          6.00         2.20              1
121  Iris-virginica          5.70         2.30          6.90         3.20              1
122  Iris-virginica          4.90         2.00          5.60         2.80              1
123  Iris-virginica          6.70         2.00          7.70         2.80              1
124  Iris-virginica          4.90         1.80          6.30         2.70              1
125  Iris-virginica          5.70         2.10          6.70         3.30              1
126  Iris-virginica          6.00         1.80          7.20         3.20              1
127  Iris-virginica          4.80         1.80          6.20         2.80              1
128  Iris-virginica          4.90         1.80          6.10         3.00              1
129  Iris-virginica          5.60         2.10          6.40         2.80              1
130  Iris-virginica          5.80         1.60          7.20         3.00              1
131  Iris-virginica          6.10         1.90          7.40         2.80              1
132  Iris-virginica          6.40         2.00          7.90         3.80              1
133  Iris-virginica          5.60         2.20          6.40         2.80              1
134  Iris-virginica          5.10         1.50          6.30         2.80              1
135  Iris-virginica          5.60         1.40          6.10         2.60              1
136  Iris-virginica          6.10         2.30          7.70         3.00              1
137  Iris-virginica          5.60         2.40          6.30         3.40              1
138  Iris-virginica          5.50         1.80          6.40         3.10              1
139  Iris-virginica          4.80         1.80          6.00         3.00              1
140  Iris-virginica          5.40         2.10          6.90         3.10              1
141  Iris-virginica          5.60         2.40          6.70         3.10              1
142  Iris-virginica          5.10         2.30          6.90         3.10              1
143  Iris-virginica          5.10         1.90          5.80         2.70              1
144  Iris-virginica          5.90         2.30          6.80         3.20              1
145  Iris-virginica          5.70         2.50          6.70         3.30              1
146  Iris-virginica          5.20         2.30          6.70         3.00              1
147  Iris-virginica          5.00         1.90          6.30         2.50              1
148  Iris-virginica          5.20         2.00          6.50         3.00              1
149  Iris-virginica          5.40         2.30          6.20         3.40              1
150  Iris-virginica          5.10         1.80          5.90         3.00              1

Jumlah outlier: 15

Data Outlier:
 id           class  petal length  petal width  sepal length  sepal width
  2     Iris-setosa        122.00       123.00        223.00       121.00
  3     Iris-setosa        234.00       241.00        121.00       142.00
  4     Iris-setosa        210.00       423.00        135.00      1423.00
  5     Iris-setosa        233.00       230.00        123.00       241.00
  6     Iris-setosa        201.00       221.00        521.00       123.00
 51 Iris-versicolor        124.00       231.00        210.00       201.00
 52 Iris-versicolor        212.00       201.00        120.00       311.00
 53 Iris-versicolor        310.00       414.00        125.00       144.00
 54 Iris-versicolor        322.00       140.00        122.00       110.00
 55 Iris-versicolor        112.00       312.00        211.00       112.00
101  Iris-virginica        121.00       134.00        122.00       112.00
102  Iris-virginica        134.00       231.00        221.00       110.00
103  Iris-virginica        123.00       234.00        113.00       112.00
104  Iris-virginica        210.00       253.00        123.00       221.00
105  Iris-virginica        240.00       124.00        333.00       211.00

Jumlah data setelah dihapus :  135

Data tidak outlier :
 id           class  petal length  petal width  sepal length  sepal width
  1     Iris-setosa          1.30         0.20          5.10         3.50
  7     Iris-setosa          1.40         0.30          4.60         3.40
  8     Iris-setosa          1.00         0.20          5.00         3.40
  9     Iris-setosa          4.00         0.20          5.00         2.90
 10     Iris-setosa          1.50         0.30          4.90         3.10
 11     Iris-setosa          1.50         0.20          5.40         3.70
 12     Iris-setosa          1.60         0.30          4.80         3.40
 13     Iris-setosa          4.30         0.10          4.80         3.00
 14     Iris-setosa          7.50         0.10          4.30         3.00
 15     Iris-setosa          1.20         0.20          5.80         4.00
 16     Iris-setosa          1.50         0.40          5.70         4.40
 17     Iris-setosa          1.30         0.40          5.40         3.90
 18     Iris-setosa          1.40         0.30          5.10         3.50
 19     Iris-setosa          1.70         0.30          5.70         3.80
 20     Iris-setosa          1.50         0.30          5.10         3.80
 21     Iris-setosa          1.70         0.20          5.40         3.40
 22     Iris-setosa          1.50         0.40          5.10         3.70
 23     Iris-setosa          1.00         0.20          4.60         3.60
 24     Iris-setosa          1.70         0.50          5.10         3.30
 25     Iris-setosa          1.90         0.20          4.80         3.40
 26     Iris-setosa          1.60         0.20          5.00         3.00
 27     Iris-setosa          1.60         0.40          5.00         3.40
 28     Iris-setosa          1.50         0.20          5.20         3.50
 29     Iris-setosa          1.40         0.20          5.20         3.40
 30     Iris-setosa          1.60         0.20          4.70         3.20
 31     Iris-setosa          1.60         0.20          4.80         3.10
 32     Iris-setosa          1.50         0.40          5.40         3.40
 33     Iris-setosa          1.50         0.10          5.20         4.10
 34     Iris-setosa          1.40         0.20          5.50         4.20
 35     Iris-setosa          1.50         0.10          4.90         3.10
 36     Iris-setosa          1.20         0.20          5.00         3.20
 37     Iris-setosa          1.30         0.20          5.50         3.50
 38     Iris-setosa          1.50         0.10          4.90         3.10
 39     Iris-setosa          1.30         0.20          4.40         3.00
 40     Iris-setosa          1.50         0.20          5.10         3.40
 41     Iris-setosa          1.30         0.30          5.00         3.50
 42     Iris-setosa          1.30         0.30          4.50         2.30
 43     Iris-setosa          1.30         0.20          4.40         3.20
 44     Iris-setosa          1.60         0.60          5.00         3.50
 45     Iris-setosa          1.90         0.40          5.10         3.80
 46     Iris-setosa          1.40         0.30          4.80         3.00
 47     Iris-setosa          1.60         0.20          5.10         3.80
 48     Iris-setosa          1.40         0.20          4.60         3.20
 49     Iris-setosa          1.50         0.20          5.30         3.70
 50     Iris-setosa          1.40         0.20          5.00         3.30
 56 Iris-versicolor          4.50         1.30          5.70         2.80
 57 Iris-versicolor          4.70         1.60          6.30         3.30
 58 Iris-versicolor          3.30         1.00          4.90         2.40
 59 Iris-versicolor          4.60         1.30          6.60         2.90
 60 Iris-versicolor          3.90         1.40          5.20         2.70
 61 Iris-versicolor          3.50         1.00          5.00         2.00
 62 Iris-versicolor          4.20         1.50          5.90         3.00
 63 Iris-versicolor          4.00         1.00          6.00         2.20
 64 Iris-versicolor          4.70         1.40          6.10         2.90
 65 Iris-versicolor          3.60         1.30          5.60         2.90
 66 Iris-versicolor          4.40         1.40          6.70         3.10
 67 Iris-versicolor          4.50         1.50          5.60         3.00
 68 Iris-versicolor          4.10         1.00          5.80         2.70
 69 Iris-versicolor          4.50         1.50          6.20         2.20
 70 Iris-versicolor          3.90         1.10          5.60         2.50
 71 Iris-versicolor          4.80         1.80          5.90         3.20
 72 Iris-versicolor          4.00         1.30          6.10         2.80
 73 Iris-versicolor          4.90         1.50          6.30         2.50
 74 Iris-versicolor          4.70         1.20          6.10         2.80
 75 Iris-versicolor          4.30         1.30          6.40         2.90
 76 Iris-versicolor          4.40         1.40          6.60         3.00
 77 Iris-versicolor          4.80         1.40          6.80         2.80
 78 Iris-versicolor          5.00         1.70          6.70         3.00
 79 Iris-versicolor          4.50         1.50          6.00         2.90
 80 Iris-versicolor          3.50         1.00          5.70         2.60
 81 Iris-versicolor          3.80         1.10          5.50         2.40
 82 Iris-versicolor          3.70         1.00          5.50         2.40
 83 Iris-versicolor          3.90         1.20          5.80         2.70
 84 Iris-versicolor          5.10         1.60          6.00         2.70
 85 Iris-versicolor          4.50         1.50          5.40         3.00
 86 Iris-versicolor          4.50         1.60          6.00         3.40
 87 Iris-versicolor          4.70         1.50          6.70         3.10
 88 Iris-versicolor          4.40         1.30          6.30         2.30
 89 Iris-versicolor          4.10         1.30          5.60         3.00
 90 Iris-versicolor          4.00         1.30          5.50         2.50
 91 Iris-versicolor          4.40         1.20          5.50         2.60
 92 Iris-versicolor          4.60         1.40          6.10         3.00
 93 Iris-versicolor          4.00         1.20          5.80         2.60
 94 Iris-versicolor          3.30         1.00          5.00         2.30
 95 Iris-versicolor          4.20         1.30          5.60         2.70
 96 Iris-versicolor          4.20         1.20          5.70         3.00
 97 Iris-versicolor          4.20         1.30          5.70         2.90
 98 Iris-versicolor          4.30         1.30          6.20         2.90
 99 Iris-versicolor          3.00         1.10          5.10         2.50
100 Iris-versicolor          4.10         1.30          5.00         2.60
106  Iris-virginica          6.60         2.10          7.60         3.00
107  Iris-virginica          4.50         1.70          4.90         2.50
108  Iris-virginica          6.30         1.80          7.30         2.90
109  Iris-virginica          5.80         1.80          6.70         2.50
110  Iris-virginica          6.10         2.50          7.20         3.60
111  Iris-virginica          5.10         2.00          6.50         3.20
112  Iris-virginica          5.30         1.90          6.40         2.70
113  Iris-virginica          5.50         2.10          6.80         3.00
114  Iris-virginica          5.00         2.00          5.70         2.50
115  Iris-virginica          5.10         2.40          5.80         2.80
116  Iris-virginica          5.30         2.30          6.40         3.20
117  Iris-virginica          5.50         1.80          6.50         3.00
118  Iris-virginica          6.70         2.20          7.70         3.80
119  Iris-virginica          6.90         2.30          7.70         2.60
120  Iris-virginica          5.00         1.50          6.00         2.20
121  Iris-virginica          5.70         2.30          6.90         3.20
122  Iris-virginica          4.90         2.00          5.60         2.80
123  Iris-virginica          6.70         2.00          7.70         2.80
124  Iris-virginica          4.90         1.80          6.30         2.70
125  Iris-virginica          5.70         2.10          6.70         3.30
126  Iris-virginica          6.00         1.80          7.20         3.20
127  Iris-virginica          4.80         1.80          6.20         2.80
128  Iris-virginica          4.90         1.80          6.10         3.00
129  Iris-virginica          5.60         2.10          6.40         2.80
130  Iris-virginica          5.80         1.60          7.20         3.00
131  Iris-virginica          6.10         1.90          7.40         2.80
132  Iris-virginica          6.40         2.00          7.90         3.80
133  Iris-virginica          5.60         2.20          6.40         2.80
134  Iris-virginica          5.10         1.50          6.30         2.80
135  Iris-virginica          5.60         1.40          6.10         2.60
136  Iris-virginica          6.10         2.30          7.70         3.00
137  Iris-virginica          5.60         2.40          6.30         3.40
138  Iris-virginica          5.50         1.80          6.40         3.10
139  Iris-virginica          4.80         1.80          6.00         3.00
140  Iris-virginica          5.40         2.10          6.90         3.10
141  Iris-virginica          5.60         2.40          6.70         3.10
142  Iris-virginica          5.10         2.30          6.90         3.10
143  Iris-virginica          5.10         1.90          5.80         2.70
144  Iris-virginica          5.90         2.30          6.80         3.20
145  Iris-virginica          5.70         2.50          6.70         3.30
146  Iris-virginica          5.20         2.30          6.70         3.00
147  Iris-virginica          5.00         1.90          6.30         2.50
148  Iris-virginica          5.20         2.00          6.50         3.00
149  Iris-virginica          5.40         2.30          6.20         3.40
150  Iris-virginica          5.10         1.80          5.90         3.00
</pre></div>
</div>
</div>
</div>
<section id="id1">
<h2><strong>1. Cara Perhitungan</strong><a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>Kode ini menggunakan Local Outlier Factor (LOF) untuk mendeteksi outlier dalam dataset berdasarkan kepadatan lokal (local density). LOF bekerja dengan cara berikut:</p>
<ol class="arabic simple">
<li><p>Menghitung jarak ke tetangga terdekat</p></li>
</ol>
<ul class="simple">
<li><p>n_neighbors=90 berarti setiap titik akan dihitung jaraknya ke 90 tetangga terdekat.</p></li>
<li><p>Jarak ini digunakan untuk mengukur kepadatan lokal dari titik tersebut.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>Membandingkan kepadatan lokal titik tersebut dengan kepadatan lokal tetangganya</p></li>
</ol>
<ul class="simple">
<li><p>Jika kepadatan lokal titik lebih rendah dibandingkan tetangga-tetangganya, maka titik ini dianggap outlier.</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p>LOF memberikan skor (-1 untuk outlier, 1 untuk normal)</p></li>
</ol>
<ul class="simple">
<li><p>Jika skor -1, maka titik dianggap outlier.</p></li>
<li><p>Jika skor 1, maka titik dianggap bukan outlier.</p></li>
</ul>
</section>
<section id="id2">
<h2><strong>2. Cara Kerja Perhitungan</strong><a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>Berikut adalah langkah-langkah utama dalam kode ini:</p>
<p>a. Mengambil Data dari Database PostgreSQL &amp; MySQL</p>
<ul class="simple">
<li><p>Menghubungkan ke PostgreSQL dan mengambil data.</p></li>
<li><p>Data dikembalikan dalam bentuk DataFrame (Pandas).</p></li>
<li><p>Menghubungkan ke MySQL dan mengambil data.</p></li>
<li><p>Data juga dikembalikan dalam bentuk DataFrame.</p></li>
</ul>
<p>b Menggabungkan Data dari PostgreSQL &amp; MySQL</p>
<ul class="simple">
<li><p>Menggabungkan kedua dataset berdasarkan id dan class.</p></li>
<li><p>Kolom Class di PostgreSQL diubah menjadi class agar sesuai dengan MySQL.</p></li>
</ul>
<p>c  Menentukan Fitur Numerik</p>
<ul class="simple">
<li><p>Hanya memilih kolom numerik untuk deteksi outlier.</p></li>
<li><p>Data dikonversi ke bentuk array NumPy agar bisa digunakan oleh LOF.</p></li>
</ul>
<p>d. Mendeteksi Outlier dengan LOF</p>
<ul class="simple">
<li><p>Menggunakan Local Outlier Factor (LOF) untuk mendeteksi outlier.</p></li>
<li><p>n_neighbors=90 berarti setiap titik dibandingkan dengan 90 tetangga terdekatnya.</p></li>
<li><p>fit_predict() mengembalikan label:</p></li>
<li><p>1  Bukan outlier (data normal)</p></li>
<li><p>-1  Outlier</p></li>
</ul>
<p>e. Menambahkan Label Outlier ke DataFrame</p>
<ul class="simple">
<li><p>Menambahkan kolom outlier_label ke dataframe.</p></li>
<li><p>Menampilkan hasil deteksi outlier dalam tabel.</p></li>
</ul>
<p>f. Menghitung Jumlah Outlier</p>
<ul class="simple">
<li><p>Menghitung jumlah data yang dianggap outlier dengan menjumlahkan label -1.</p></li>
</ul>
<p>g. Memisahkan Data Outlier dan Non-Outlier</p>
<ul class="simple">
<li><p>Data yang bukan outlier (label == 1) disimpan di df_filtered.</p></li>
<li><p>Data yang outlier (label == -1) disimpan di outliers.</p></li>
<li><p>Kolom outlier_label dihapus agar tidak mengganggu analisis selanjutnya.</p></li>
</ul>
<p>h. Menampilkan Hasil</p>
<ul class="simple">
<li><p>Menampilkan semua data yang dianggap outlier.</p></li>
<li><p>Menampilkan jumlah data setelah outlier dihapus.</p></li>
<li><p>Menampilkan data yang bukan outlier.</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="menghitung-akurasi-data">
<h1><strong>Menghitung Akurasi Data</strong><a class="headerlink" href="#menghitung-akurasi-data" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df_merged</span><span class="p">[</span><span class="n">feature_columns</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span>

<span class="c1"># Encode label kelas ke numerik</span>
<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">y_encoded</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Menerapkan LOF untuk deteksi outlier</span>
<span class="n">lof</span> <span class="o">=</span> <span class="n">LocalOutlierFactor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">contamination</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">outlier_labels</span> <span class="o">=</span> <span class="n">lof</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;outlier&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outlier_labels</span>

<span class="c1"># Pisahkan data dengan outlier dan tanpa outlier</span>
<span class="n">df_cleaned</span> <span class="o">=</span> <span class="n">df_merged</span><span class="p">[</span><span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;outlier&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;outlier&quot;</span><span class="p">])</span>

<span class="c1"># Membagi data menjadi training (80%) dan testing (20%)</span>
<span class="n">X_train_all</span><span class="p">,</span> <span class="n">X_test_all</span><span class="p">,</span> <span class="n">y_train_all</span><span class="p">,</span> <span class="n">y_test_all</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y_encoded</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="n">X_train_clean</span><span class="p">,</span> <span class="n">X_test_clean</span><span class="p">,</span> <span class="n">y_train_clean</span><span class="p">,</span> <span class="n">y_test_clean</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">df_cleaned</span><span class="p">[</span><span class="n">feature_columns</span><span class="p">],</span>
    <span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_cleaned</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]),</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># Pipeline dengan StandardScaler dan KNN</span>
<span class="n">knn_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s2">&quot;knn&quot;</span><span class="p">,</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">11</span><span class="p">))</span>
<span class="p">])</span>

<span class="c1"># Latih model pada data dengan outlier</span>
<span class="n">knn_pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_all</span><span class="p">,</span> <span class="n">y_train_all</span><span class="p">)</span>
<span class="n">y_pred_all</span> <span class="o">=</span> <span class="n">knn_pipeline</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_all</span><span class="p">)</span>
<span class="n">accuracy_all</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test_all</span><span class="p">,</span> <span class="n">y_pred_all</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Akurasi dengan outlier:&quot;</span><span class="p">,</span> <span class="n">accuracy_all</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test_all</span><span class="p">,</span> <span class="n">y_pred_all</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>

<span class="c1"># Latih model pada data tanpa outlier</span>
<span class="n">knn_pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_clean</span><span class="p">,</span> <span class="n">y_train_clean</span><span class="p">)</span>
<span class="n">y_pred_clean</span> <span class="o">=</span> <span class="n">knn_pipeline</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_clean</span><span class="p">)</span>
<span class="n">accuracy_clean</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test_clean</span><span class="p">,</span> <span class="n">y_pred_clean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Akurasi tanpa outlier:&quot;</span><span class="p">,</span> <span class="n">accuracy_clean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test_clean</span><span class="p">,</span> <span class="n">y_pred_clean</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Akurasi dengan outlier: 0.9333333333333333
                 precision    recall  f1-score   support

    Iris-setosa       0.90      0.90      0.90        10
Iris-versicolor       0.90      1.00      0.95         9
 Iris-virginica       1.00      0.91      0.95        11

       accuracy                           0.93        30
      macro avg       0.93      0.94      0.93        30
   weighted avg       0.94      0.93      0.93        30

Akurasi tanpa outlier: 0.9259259259259259
                 precision    recall  f1-score   support

    Iris-setosa       1.00      1.00      1.00        12
Iris-versicolor       1.00      0.71      0.83         7
 Iris-virginica       0.80      1.00      0.89         8

       accuracy                           0.93        27
      macro avg       0.93      0.90      0.91        27
   weighted avg       0.94      0.93      0.92        27
</pre></div>
</div>
</div>
</div>
<p>Kode di atas melakukan deteksi outlier menggunakan <strong>Local Outlier Factor (LOF)</strong> dan membandingkan performa model <strong>K-Nearest Neighbors (KNN)</strong> pada data dengan dan tanpa outlier. Pertama, data fitur numerik (<code class="docutils literal notranslate"><span class="pre">X</span></code>) dan label kelas (<code class="docutils literal notranslate"><span class="pre">y</span></code>) diekstrak dari dataset yang telah digabungkan (<code class="docutils literal notranslate"><span class="pre">df_merged</span></code>). Label kelas kemudian dikonversi menjadi nilai numerik menggunakan <strong>LabelEncoder</strong>. LOF digunakan untuk mengidentifikasi outlier dengan mempertimbangkan <strong>90 tetangga terdekat</strong> dan menetapkan <strong>10% dari data sebagai outlier</strong>. Data yang tidak terdeteksi sebagai outlier disimpan dalam <code class="docutils literal notranslate"><span class="pre">df_cleaned</span></code>, sementara data asli tetap digunakan untuk perbandingan.</p>
<p>Selanjutnya, dataset dibagi menjadi <strong>data pelatihan (80%) dan pengujian (20%)</strong>, baik untuk dataset dengan outlier (<code class="docutils literal notranslate"><span class="pre">X_train_all,</span> <span class="pre">X_test_all</span></code>) maupun tanpa outlier (<code class="docutils literal notranslate"><span class="pre">X_train_clean,</span> <span class="pre">X_test_clean</span></code>). Model <strong>KNN (dengan 11 tetangga)</strong> diterapkan dalam <strong>pipeline yang mencakup StandardScaler</strong> untuk menormalkan data sebelum pelatihan. Model pertama dilatih dengan <strong>data yang masih mengandung outlier</strong>, lalu diuji, dan hasil akurasinya dicetak. Kemudian, model yang sama dilatih menggunakan <strong>data tanpa outlier</strong>, dan akurasinya dibandingkan. Evaluasi model dilakukan dengan <strong>classification report</strong>, yang menunjukkan metrik seperti <strong>precision, recall, dan F1-score</strong>. Hasil akhirnya memungkinkan perbandingan dampak penghapusan outlier terhadap performa klasifikasi KNN.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="visualisasi-data">
<h1><strong>Visualisasi Data</strong><a class="headerlink" href="#visualisasi-data" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>

<span class="c1"># Ambil dua fitur utama untuk visualisasi decision boundary</span>
<span class="n">feature_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;petal length&quot;</span><span class="p">,</span> <span class="s2">&quot;petal width&quot;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_cleaned</span><span class="p">[</span><span class="n">feature_columns</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_cleaned</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span>

<span class="c1"># Encode label kelas ke numerik</span>
<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">y_encoded</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># Ubah nama kelas jadi angka</span>

<span class="c1"># Membagi data menjadi training (80%) dan testing (20%)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y_encoded</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># Pipeline dengan KNN dan StandardScaler</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s2">&quot;knn&quot;</span><span class="p">,</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">11</span><span class="p">))</span>
<span class="p">])</span>

<span class="c1"># Pelatihan model KNN</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Evaluasi model</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Akurasi: </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>

<span class="c1"># Visualisasi Decision Boundary</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">weights</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="s2">&quot;distance&quot;</span><span class="p">)):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">knn__weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">disp</span> <span class="o">=</span> <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
        <span class="n">clf</span><span class="p">,</span>
        <span class="n">X_test</span><span class="p">,</span>
        <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
        <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
        <span class="n">xlabel</span><span class="o">=</span><span class="n">feature_columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">ylabel</span><span class="o">=</span><span class="n">feature_columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">scatter</span> <span class="o">=</span> <span class="n">disp</span><span class="o">.</span><span class="n">ax_</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">c</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span>
    <span class="p">)</span>
    <span class="n">disp</span><span class="o">.</span><span class="n">ax_</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span>
        <span class="n">scatter</span><span class="o">.</span><span class="n">legend_elements</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span>
        <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower left&quot;</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Classes&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">disp</span><span class="o">.</span><span class="n">ax_</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;3-Class classification</span><span class="se">\n</span><span class="s2">(k=</span><span class="si">{</span><span class="n">clf</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">n_neighbors</span><span class="si">}</span><span class="s2">, weights=</span><span class="si">{</span><span class="n">weights</span><span class="si">!r}</span><span class="s2">)&quot;</span>
    <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Akurasi: 0.9629629629629629
                 precision    recall  f1-score   support

    Iris-setosa       1.00      1.00      1.00        12
Iris-versicolor       1.00      0.86      0.92         7
 Iris-virginica       0.89      1.00      0.94         8

       accuracy                           0.96        27
      macro avg       0.96      0.95      0.95        27
   weighted avg       0.97      0.96      0.96        27
</pre></div>
</div>
<img alt="_images/05c3bfc31ab15345baa30826508215300a507699b571fbea050ae84b16918ff6.png" src="_images/05c3bfc31ab15345baa30826508215300a507699b571fbea050ae84b16918ff6.png" />
</div>
</div>
<p>Kode di atas bertujuan untuk melakukan klasifikasi data menggunakan algoritma K-Nearest Neighbors (KNN) dan memvisualisasikan decision boundary dari model yang telah dilatih. Pertama, kode mengambil dua fitur utama, yaitu petal length dan petal width, dari dataset df_cleaned, yang kemudian digunakan sebagai variabel prediktor (X). Sementara itu, kolom class digunakan sebagai target (y), yang kemudian dikonversi ke bentuk numerik menggunakan LabelEncoder. Setelah itu, data dibagi menjadi training set (80%) dan testing set (20%) menggunakan train_test_split.</p>
<p>Selanjutnya, pipeline dibuat menggunakan StandardScaler (untuk normalisasi data) dan KNeighborsClassifier dengan jumlah tetangga (k=11). Model kemudian dilatih (fit) menggunakan data training, dan diuji (predict) pada data testing. Hasil evaluasi ditampilkan dengan akurasi serta classification report yang mencakup metrik seperti precision, recall, dan F1-score.</p>
<p>Bagian terakhir dari kode berfokus pada visualisasi decision boundary untuk memahami bagaimana model KNN membagi area klasifikasi berdasarkan dua fitur utama. Dua jenis bobot (uniform dan distance) digunakan untuk membandingkan pengaruhnya dalam membentuk decision boundary. DecisionBoundaryDisplay digunakan untuk menampilkan hasil prediksi model dalam bentuk kontur warna yang menggambarkan area klasifikasi, sementara data asli ditampilkan sebagai titik-titik scatterplot dengan warna yang sesuai dengan kelasnya. Dengan visualisasi ini, pengguna dapat memahami bagaimana model KNN membedakan kelas berdasarkan pola distribusi data.</p>
<section id="persiapan-data">
<h2>1. <strong>Persiapan Data</strong><a class="headerlink" href="#persiapan-data" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>emilihan fitur: Hanya dua fitur digunakan, yaitu petal length dan petal width, agar dapat divisualisasikan dalam decision boundary.</p></li>
<li><p>Pemilihan target: Kolom class digunakan sebagai label target untuk klasifikasi.</p></li>
<li><p>Karena algoritma KNN bekerja dengan angka, LabelEncoder digunakan untuk mengonversi nama kelas (string) menjadi angka.</p></li>
</ul>
</section>
<section id="membagi-data-training-dan-testing">
<h2><strong>2. Membagi Data Training dan Testing</strong><a class="headerlink" href="#membagi-data-training-dan-testing" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Data dibagi menjadi 80% training set dan 20% testing set.</p></li>
<li><p>random_state=42 memastikan pembagian data tetap konsisten setiap kali kode dijalankan.</p></li>
</ul>
</section>
<section id="pembuatan-dan-pelatihan-model-knn">
<h2><strong>3. Pembuatan dan Pelatihan Model KNN</strong><a class="headerlink" href="#pembuatan-dan-pelatihan-model-knn" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Pipeline digunakan untuk menghubungkan dua tahapan utama:</p></li>
</ul>
<ol class="arabic simple">
<li><p>StandardScaler: Melakukan normalisasi agar fitur memiliki distribusi yang seragam.</p></li>
<li><p>KNeighborsClassifier: Model KNN dengan k=11, artinya setiap prediksi didasarkan pada 11 tetangga terdekat.</p></li>
</ol>
<ul class="simple">
<li><p>Model KNN dilatih menggunakan data training.</p></li>
</ul>
</section>
<section id="evaluasi-model">
<h2><strong>4. Evaluasi Model</strong><a class="headerlink" href="#evaluasi-model" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Prediksi dilakukan pada data testing.</p></li>
<li><p>Evaluasi model dilakukan dengan:</p></li>
</ul>
<ol class="arabic simple">
<li><p>Akurasi: Proporsi prediksi yang benar terhadap jumlah total data uji.</p></li>
<li><p>Classification Report: Menampilkan metrik seperti precision, recall, dan F1-score untuk setiap kelas.</p></li>
</ol>
</section>
<section id="visualisasi-decision-boundary">
<h2><strong>5. Visualisasi Decision Boundary</strong><a class="headerlink" href="#visualisasi-decision-boundary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Dua subplot dibuat untuk membandingkan decision boundary dari dua skenario yang berbeda.</p></li>
<li><p>Dua jenis bobot (weights) diuji:</p></li>
</ul>
<ol class="arabic simple">
<li><p>uniform  Semua tetangga memiliki bobot yang sama.</p></li>
<li><p>distance  Tetangga yang lebih dekat memiliki bobot lebih besar.</p></li>
</ol>
<ul class="simple">
<li><p>Model dilatih ulang (fit) untuk masing-masing skenario.</p></li>
<li><p>DecisionBoundaryDisplay digunakan untuk membuat visualisasi decision boundary model KNN.</p></li>
<li><p>Warna pada grafik menunjukkan area klasifikasi berdasarkan prediksi model.</p></li>
<li><p>Data training ditampilkan sebagai titik-titik (scatter plot) di atas decision boundary.</p></li>
<li><p>Legenda ditambahkan agar warna dalam grafik dapat dikaitkan dengan kelas tertentu.</p></li>
<li><p>Judul ditambahkan untuk menjelaskan parameter yang digunakan dalam model.</p></li>
<li><p>Grafik ditampilkan untuk membantu memahami bagaimana model KNN membagi area klasifikasi berdasarkan fitur.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">psycopg2</span>
<span class="kn">import</span> <span class="nn">pymysql</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">LocalOutlierFactor</span><span class="p">,</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>


<span class="c1"># Ambil dua fitur utama untuk visualisasi decision boundary</span>
<span class="n">feature_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sepal length&quot;</span><span class="p">,</span> <span class="s2">&quot;sepal width&quot;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_merged</span><span class="p">[</span><span class="n">feature_columns</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span>

<span class="c1"># Encode label kelas ke numerik</span>
<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">y_encoded</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># Mengubah nama kelas menjadi angka</span>

<span class="c1"># Menerapkan LOF untuk deteksi outlier</span>
<span class="n">lof</span> <span class="o">=</span> <span class="n">LocalOutlierFactor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">contamination</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">outlier_labels</span> <span class="o">=</span> <span class="n">lof</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;outlier&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outlier_labels</span>

<span class="c1"># Menghapus data yang terdeteksi sebagai outlier</span>
<span class="n">df_cleaned</span> <span class="o">=</span> <span class="n">df_merged</span><span class="p">[</span><span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;outlier&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;outlier&quot;</span><span class="p">])</span>

<span class="c1"># Membagi data menjadi training (80%) dan testing (20%)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">df_cleaned</span><span class="p">[</span><span class="n">feature_columns</span><span class="p">],</span>
    <span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_cleaned</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]),</span>  <span class="c1"># Pastikan target dalam bentuk numerik</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># Pipeline dengan KNN dan StandardScaler</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s2">&quot;knn&quot;</span><span class="p">,</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">11</span><span class="p">))</span>
<span class="p">])</span>

<span class="c1"># Pelatihan model KNN</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Evaluasi model</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Akurasi: </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>

<span class="c1"># Visualisasi Decision Boundary</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">weights</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="s2">&quot;distance&quot;</span><span class="p">)):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">knn__weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">disp</span> <span class="o">=</span> <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
        <span class="n">clf</span><span class="p">,</span>
        <span class="n">X_test</span><span class="p">,</span>
        <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">,</span>
        <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
        <span class="n">xlabel</span><span class="o">=</span><span class="n">feature_columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">ylabel</span><span class="o">=</span><span class="n">feature_columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">scatter</span> <span class="o">=</span> <span class="n">disp</span><span class="o">.</span><span class="n">ax_</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">c</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span>
    <span class="p">)</span>
    <span class="n">disp</span><span class="o">.</span><span class="n">ax_</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span>
        <span class="n">scatter</span><span class="o">.</span><span class="n">legend_elements</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span>
        <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower left&quot;</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Classes&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">disp</span><span class="o">.</span><span class="n">ax_</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;3-Class classification</span><span class="se">\n</span><span class="s2">(k=</span><span class="si">{</span><span class="n">clf</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">n_neighbors</span><span class="si">}</span><span class="s2">, weights=</span><span class="si">{</span><span class="n">weights</span><span class="si">!r}</span><span class="s2">)&quot;</span>
    <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Akurasi: 0.8148148148148148
                 precision    recall  f1-score   support

    Iris-setosa       1.00      1.00      1.00        12
Iris-versicolor       0.62      0.71      0.67         7
 Iris-virginica       0.71      0.62      0.67         8

       accuracy                           0.81        27
      macro avg       0.78      0.78      0.78        27
   weighted avg       0.82      0.81      0.81        27
</pre></div>
</div>
<img alt="_images/032cfec4ad686667062f5abc8ee11ad7fc31b578d7a4f5cc0f8596cbee505162.png" src="_images/032cfec4ad686667062f5abc8ee11ad7fc31b578d7a4f5cc0f8596cbee505162.png" />
</div>
</div>
<p>melakukan klasifikasi menggunakan algoritma K-Nearest Neighbors (KNN) setelah terlebih dahulu menghilangkan outlier menggunakan Local Outlier Factor (LOF). Berikut adalah langkah-langkah utama yang dilakukan dalam kode ini:</p>
<p>Pertama, dua fitur utama yang digunakan untuk klasifikasi adalah sepal length dan sepal width. Label kelas dikonversi ke dalam bentuk numerik menggunakan LabelEncoder, sehingga model dapat memprosesnya dengan lebih baik. Setelah itu, Local Outlier Factor (LOF) diterapkan dengan n_neighbors=20 dan contamination=0.1, yang berarti 10% dari data akan dideteksi sebagai outlier. LOF bekerja dengan mengukur kepadatan lokal data, di mana data yang memiliki kepadatan berbeda dari tetangganya akan diklasifikasikan sebagai outlier. Data yang terdeteksi sebagai outlier kemudian dihapus sebelum proses pelatihan model dilakukan.</p>
<p>Setelah pembersihan data, dataset dibagi menjadi 80% data training dan 20% data testing menggunakan train_test_split(). Model KNN dibuat menggunakan Pipeline, yang menghubungkan StandardScaler (untuk normalisasi data) dan KNeighborsClassifier dengan n_neighbors=11. Model kemudian dilatih menggunakan data training.</p>
<p>Untuk evaluasi, model diuji pada data testing dan hasil klasifikasi diukur menggunakan akurasi dan classification report, yang mencakup metrik seperti precision, recall, dan F1-score. Terakhir, kode membuat visualisasi decision boundary untuk melihat bagaimana model KNN mengklasifikasikan data berdasarkan dua skenario bobot berbeda: uniform (bobot yang sama untuk semua tetangga) dan distance (tetangga yang lebih dekat memiliki bobot lebih besar). Decision boundary divisualisasikan dengan menggunakan warna yang menunjukkan area klasifikasi model.</p>
<p>Secara keseluruhan, kode ini bertujuan untuk meningkatkan akurasi klasifikasi dengan menghilangkan outlier sebelum pelatihan model, serta memvisualisasikan bagaimana model membagi data ke dalam kelas-kelas berdasarkan fitur yang digunakan.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Pilih dua fitur untuk scatter plot</span>
<span class="n">x_feature</span> <span class="o">=</span> <span class="s2">&quot;petal length&quot;</span>
<span class="n">y_feature</span> <span class="o">=</span> <span class="s2">&quot;petal width&quot;</span>

<span class="c1"># Warna berdasarkan kelas</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Iris-setosa&quot;</span><span class="p">:</span> <span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;Iris-versicolor&quot;</span><span class="p">:</span> <span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="s2">&quot;Iris-virginica&quot;</span><span class="p">:</span> <span class="s2">&quot;red&quot;</span><span class="p">}</span>
<span class="n">df_cleaned</span><span class="p">[</span><span class="s2">&quot;color&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_cleaned</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">colors</span><span class="p">)</span>

<span class="c1"># Plot scatter dengan ukuran (s) dan warna (c)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_cleaned</span><span class="p">[</span><span class="n">x_feature</span><span class="p">],</span> <span class="n">df_cleaned</span><span class="p">[</span><span class="n">y_feature</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">df_cleaned</span><span class="p">[</span><span class="s2">&quot;color&quot;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">x_feature</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">y_feature</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Scatter Plot dengan Warna Berdasarkan Class&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/046c3cca1d22cbf6d77d421685787318e4ebd0b7f6a713e602467cd19a7189a9.png" src="_images/046c3cca1d22cbf6d77d421685787318e4ebd0b7f6a713e602467cd19a7189a9.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Pilih dua fitur untuk scatter plot</span>
<span class="n">x_feature</span> <span class="o">=</span> <span class="s2">&quot;sepal length&quot;</span>
<span class="n">y_feature</span> <span class="o">=</span> <span class="s2">&quot;sepal width&quot;</span>

<span class="c1"># Warna berdasarkan kelas</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Iris-setosa&quot;</span><span class="p">:</span> <span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;Iris-versicolor&quot;</span><span class="p">:</span> <span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="s2">&quot;Iris-virginica&quot;</span><span class="p">:</span> <span class="s2">&quot;red&quot;</span><span class="p">}</span>
<span class="n">df_cleaned</span><span class="p">[</span><span class="s2">&quot;color&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_cleaned</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">colors</span><span class="p">)</span>

<span class="c1"># Plot scatter dengan ukuran (s) dan warna (c)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_cleaned</span><span class="p">[</span><span class="n">x_feature</span><span class="p">],</span> <span class="n">df_cleaned</span><span class="p">[</span><span class="n">y_feature</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">df_cleaned</span><span class="p">[</span><span class="s2">&quot;color&quot;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">x_feature</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">y_feature</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Scatter Plot dengan Warna Berdasarkan Class&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8befdbe26f902908d869ab91681efb10e959200f659b94c14a3e5c5ccb3bb09b.png" src="_images/8befdbe26f902908d869ab91681efb10e959200f659b94c14a3e5c5ccb3bb09b.png" />
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="implementasi-naive-bayes-pad-data">
<h1><strong>Implementasi Naive Bayes pad Data</strong><a class="headerlink" href="#implementasi-naive-bayes-pad-data" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">confusion_matrix</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Asumsikan df_merged sudah ada dari kode sebelumnya</span>

<span class="c1"># Encode label kelas ke numerik</span>
<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;class_encoded&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">])</span>

<span class="c1"># Data dengan outlier</span>
<span class="n">X_all</span> <span class="o">=</span> <span class="n">df_merged</span><span class="p">[</span><span class="n">feature_columns</span><span class="p">]</span>
<span class="n">y_all</span> <span class="o">=</span> <span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;class_encoded&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>  <span class="c1"># Pastikan dalam bentuk array numpy</span>

<span class="c1"># Data tanpa outlier</span>
<span class="n">df_cleaned</span> <span class="o">=</span> <span class="n">df_merged</span><span class="p">[</span><span class="n">df_merged</span><span class="p">[</span><span class="s2">&quot;outlier&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;outlier&quot;</span><span class="p">])</span>
<span class="n">X_clean</span> <span class="o">=</span> <span class="n">df_cleaned</span><span class="p">[</span><span class="n">feature_columns</span><span class="p">]</span>
<span class="n">y_clean</span> <span class="o">=</span> <span class="n">df_cleaned</span><span class="p">[</span><span class="s2">&quot;class_encoded&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>  <span class="c1"># Pastikan dalam bentuk array numpy</span>

<span class="c1"># Split data dengan outlier</span>
<span class="n">X_train_all</span><span class="p">,</span> <span class="n">X_test_all</span><span class="p">,</span> <span class="n">y_train_all</span><span class="p">,</span> <span class="n">y_test_all</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_all</span><span class="p">,</span> <span class="n">y_all</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Split data tanpa outlier</span>
<span class="n">X_train_clean</span><span class="p">,</span> <span class="n">X_test_clean</span><span class="p">,</span> <span class="n">y_train_clean</span><span class="p">,</span> <span class="n">y_test_clean</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_clean</span><span class="p">,</span> <span class="n">y_clean</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Inisialisasi model Naive Bayes</span>
<span class="n">gnb</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>

<span class="c1"># Latih dan uji model dengan outlier</span>
<span class="n">y_pred_all</span> <span class="o">=</span> <span class="n">gnb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_all</span><span class="p">,</span> <span class="n">y_train_all</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_all</span><span class="p">)</span>
<span class="n">mislabeled_all</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_test_all</span> <span class="o">!=</span> <span class="n">y_pred_all</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">accuracy_all</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test_all</span><span class="p">,</span> <span class="n">y_pred_all</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of mislabeled points with outliers out of a total </span><span class="si">%d</span><span class="s2"> points : </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">X_test_all</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mislabeled_all</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy with outliers: </span><span class="si">%.2f%%</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy_all</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>

<span class="c1"># Menampilkan label yang salah pada data dengan outlier</span>
<span class="n">mislabeled_indices_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_test_all</span> <span class="o">!=</span> <span class="n">y_pred_all</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mislabeled points with outliers:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">mislabeled_indices_all</span><span class="p">:</span>
    <span class="n">true_label</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">y_test_all</span><span class="p">[</span><span class="n">i</span><span class="p">])])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">predicted_label</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">y_pred_all</span><span class="p">[</span><span class="n">i</span><span class="p">])])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Index: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, True Label: </span><span class="si">{</span><span class="n">true_label</span><span class="si">}</span><span class="s2">, Predicted: </span><span class="si">{</span><span class="n">predicted_label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="c1"># Latih dan uji model tanpa outlier</span>
<span class="n">y_pred_clean</span> <span class="o">=</span> <span class="n">gnb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_clean</span><span class="p">,</span> <span class="n">y_train_clean</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_clean</span><span class="p">)</span>
<span class="n">mislabeled_clean</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_test_clean</span> <span class="o">!=</span> <span class="n">y_pred_clean</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">accuracy_clean</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test_clean</span><span class="p">,</span> <span class="n">y_pred_clean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of mislabeled points without outliers out of a total </span><span class="si">%d</span><span class="s2"> points : </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">X_test_clean</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mislabeled_clean</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy without outliers: </span><span class="si">%.2f%%</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy_clean</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>

<span class="c1"># Menampilkan label yang salah pada data tanpa outlier</span>
<span class="n">mislabeled_indices_clean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_test_clean</span> <span class="o">!=</span> <span class="n">y_pred_clean</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mislabeled points without outliers:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">mislabeled_indices_clean</span><span class="p">:</span>
    <span class="n">true_label</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">y_test_clean</span><span class="p">[</span><span class="n">i</span><span class="p">])])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">predicted_label</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">y_pred_clean</span><span class="p">[</span><span class="n">i</span><span class="p">])])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Index: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, True Label: </span><span class="si">{</span><span class="n">true_label</span><span class="si">}</span><span class="s2">, Predicted: </span><span class="si">{</span><span class="n">predicted_label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Visualisasi Confusion Matrix</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test_all</span><span class="p">,</span> <span class="n">y_pred_all</span><span class="p">),</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Blues&quot;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix with Outliers&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted Label&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;True Label&quot;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test_clean</span><span class="p">,</span> <span class="n">y_pred_clean</span><span class="p">),</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Blues&quot;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix without Outliers&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted Label&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;True Label&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of mislabeled points with outliers out of a total 30 points : 20
Accuracy with outliers: 33.33%
Mislabeled points with outliers:
Index: 0, True Label: Iris-versicolor, Predicted: Iris-virginica
Index: 1, True Label: Iris-setosa, Predicted: Iris-virginica
Index: 3, True Label: Iris-versicolor, Predicted: Iris-virginica
Index: 4, True Label: Iris-versicolor, Predicted: Iris-virginica
Index: 5, True Label: Iris-setosa, Predicted: Iris-virginica
Index: 6, True Label: Iris-versicolor, Predicted: Iris-virginica
Index: 8, True Label: Iris-versicolor, Predicted: Iris-virginica
Index: 9, True Label: Iris-versicolor, Predicted: Iris-virginica
Index: 11, True Label: Iris-setosa, Predicted: Iris-virginica
Index: 12, True Label: Iris-setosa, Predicted: Iris-virginica
Index: 13, True Label: Iris-setosa, Predicted: Iris-virginica
Index: 14, True Label: Iris-setosa, Predicted: Iris-virginica
Index: 15, True Label: Iris-versicolor, Predicted: Iris-virginica
Index: 16, True Label: Iris-virginica, Predicted: Iris-setosa
Index: 17, True Label: Iris-versicolor, Predicted: Iris-virginica
Index: 18, True Label: Iris-versicolor, Predicted: Iris-virginica
Index: 20, True Label: Iris-setosa, Predicted: Iris-virginica
Index: 22, True Label: Iris-setosa, Predicted: Iris-virginica
Index: 28, True Label: Iris-setosa, Predicted: Iris-virginica
Index: 29, True Label: Iris-setosa, Predicted: Iris-virginica

Number of mislabeled points without outliers out of a total 27 points : 4
Accuracy without outliers: 85.19%
Mislabeled points without outliers:
Index: 0, True Label: Iris-virginica, Predicted: Iris-versicolor
Index: 1, True Label: Iris-versicolor, Predicted: Iris-virginica
Index: 14, True Label: Iris-virginica, Predicted: Iris-versicolor
Index: 25, True Label: Iris-virginica, Predicted: Iris-versicolor
</pre></div>
</div>
<img alt="_images/68973bee4dbd980af8b140e05fdf0426bdb05824327453c581d92203bcf660ab.png" src="_images/68973bee4dbd980af8b140e05fdf0426bdb05824327453c581d92203bcf660ab.png" />
</div>
</div>
<section id="id3">
<h2><strong>1. Persiapan Data</strong><a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Label encoding: Kolom class dikonversi ke bentuk numerik menggunakan LabelEncoder, sehingga dapat digunakan dalam model klasifikasi.</p></li>
<li><p>Pembagian dataset:</p></li>
</ul>
<ol class="arabic simple">
<li><p>Dataset dengan outlier (X_all, y_all): Menggunakan seluruh data tanpa filter.</p></li>
<li><p>Dataset tanpa outlier (X_clean, y_clean): Menghapus data yang dideteksi sebagai outlier oleh Local Outlier Factor (LOF).</p></li>
</ol>
<ul class="simple">
<li><p>Data splitting: Kedua dataset dibagi menjadi training (80%) dan testing (20%) menggunakan train_test_split().</p></li>
</ul>
</section>
<section id="pelatihan-dan-evaluasi-model">
<h2><strong>2. Pelatihan dan Evaluasi Model</strong><a class="headerlink" href="#pelatihan-dan-evaluasi-model" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Inisialisasi model Gaussian Nave Bayes (gnb):</p></li>
<li><p>Model dilatih menggunakan fit(X_train, y_train).</p></li>
<li><p>Prediksi dilakukan dengan predict(X_test).</p></li>
</ul>
<p><strong>Evaluasi dengan Outlier</strong></p>
<ul class="simple">
<li><p>Model diuji pada dataset dengan outlier (X_test_all, y_test_all).</p></li>
<li><p>Menghitung jumlah kesalahan klasifikasi (mislabeled_all) sebagai jumlah sampel yang diprediksi salah.</p></li>
<li><p>Menghitung akurasi menggunakan accuracy_score().</p></li>
<li><p>Menampilkan label yang salah diklasifikasikan, dengan mencocokkan hasil prediksi terhadap label aslinya.</p></li>
</ul>
<p><strong>Evaluasi tanpa Outlier</strong></p>
<ul class="simple">
<li><p>Model diuji ulang dengan dataset yang sudah dibersihkan (X_test_clean, y_test_clean).</p></li>
<li><p>Perhitungan metrik yang sama dilakukan untuk membandingkan hasil klasifikasi sebelum dan sesudah penghapusan outlier.</p></li>
</ul>
</section>
<section id="visualisasi-confusion-matrix">
<h2><strong>3. Visualisasi Confusion Matrix</strong><a class="headerlink" href="#visualisasi-confusion-matrix" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Dua confusion matrix dibuat menggunakan sns.heatmap():</p></li>
<li><p>Sebelah kiri: Confusion matrix untuk dataset dengan outlier.</p></li>
<li><p>Sebelah kanan: Confusion matrix untuk dataset tanpa outlier.</p></li>
<li><p>Interpretasi:</p></li>
<li><p>Confusion matrix menunjukkan jumlah sampel yang diklasifikasikan benar dan salah untuk setiap kelas.</p></li>
<li><p>Jika confusion matrix tanpa outlier lebih bersih (lebih sedikit kesalahan klasifikasi), berarti penghapusan outlier meningkatkan performa model.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Tugas2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><strong>OUTLINER DETEKSI</strong></p>
      </div>
    </a>
    <a class="right-next"
       href="LOF.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><strong>Deteksi Outlier dengan metode Local Outlier Factor (LOF) dalam Data Understanding</strong></p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#"><strong>Deteksi Outlier dengan K-Nearest Neighbors (KNN) dalam Data Understanding</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mengapa-knn-bisa-digunakan-untuk-deteksi-outlier">1. Mengapa KNN Bisa Digunakan untuk Deteksi Outlier?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#langkah-langkah-deteksi-outlier-dengan-knn">2. Langkah-Langkah Deteksi Outlier dengan KNN</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#code-untuk-menampilkan-semua-data-dan-grafiknya"><strong>Code Untuk Menampilkan Semua Data dan Grafiknya</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cara-perhitungan"><strong>1. Cara Perhitungan</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cara-kerja-perhitungan"><strong>2. Cara Kerja Perhitungan</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kegunaan-kode"><strong>3. Kegunaan Kode</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#memisahkan-outlier-dari-data"><strong>Memisahkan Outlier dari Data</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><strong>1. Cara Perhitungan</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><strong>2. Cara Kerja Perhitungan</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#menghitung-akurasi-data"><strong>Menghitung Akurasi Data</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#visualisasi-data"><strong>Visualisasi Data</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#persiapan-data">1. <strong>Persiapan Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#membagi-data-training-dan-testing"><strong>2. Membagi Data Training dan Testing</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pembuatan-dan-pelatihan-model-knn"><strong>3. Pembuatan dan Pelatihan Model KNN</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluasi-model"><strong>4. Evaluasi Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualisasi-decision-boundary"><strong>5. Visualisasi Decision Boundary</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#implementasi-naive-bayes-pad-data"><strong>Implementasi Naive Bayes pad Data</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><strong>1. Persiapan Data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pelatihan-dan-evaluasi-model"><strong>2. Pelatihan dan Evaluasi Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualisasi-confusion-matrix"><strong>3. Visualisasi Confusion Matrix</strong></a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Tegar Firmandani
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
       Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>